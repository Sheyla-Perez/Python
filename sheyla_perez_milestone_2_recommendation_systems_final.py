# -*- coding: utf-8 -*-
"""Sheyla_Perez_Milestone_2_Recommendation_Systems_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pn9q68R5ahG3cfiuKpT5t_AzWm586lER

# **Music Recommendation System**

# **Milestone 2**

Now that we have explored the data, let's apply different algorithms to build recommendation systems.

**Note:** Use the shorter version of the data, i.e., the data after the cutoffs as used in Milestone 1.

## **Load the dataset**
"""

# Used to ignore the warning given as output of the code
import warnings
warnings.filterwarnings('ignore')

# Basic libraries of python for numeric and dataframe computations
import numpy as np
import pandas as pd

# Basic library for data visualization
import matplotlib.pyplot as plt

# Slightly advanced library for data visualization
import seaborn as sns

# To compute the cosine similarity between two vectors
from sklearn.metrics.pairwise import cosine_similarity

# A dictionary output that does not raise a key error
from collections import defaultdict

# A performance metrics in sklearn
from sklearn.metrics import mean_squared_error

from sklearn import preprocessing

from google.colab import drive
drive.mount('/content/drive')

# Load the dataset you have saved at the end of milestone 1
result_final = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/result_final.csv')

"""### **Popularity-Based Recommendation Systems**

Let's take the count and sum of play counts of the songs and build the popularity recommendation systems based on the sum of play counts.
"""

# Calculating average play_count
average_count = result_final.groupby('song_id').mean()['play_count']       # Hint: Use groupby function on the song_id column
average_count

result_final

# Calculating the frequency a song is played
play_freq = result_final.groupby('song_id').count()['play_count']
play_freq

# Making a dataframe with the average_count and play_freq
final_play = pd.DataFrame({'average_count':average_count,'play_count':play_freq})
# Let us see the first five records of the final_play dataset
final_play.head(5)

"""Now, let's create a function to find the top n songs for a recommendation based on the average play count of song. We can also add a threshold for a minimum number of playcounts for a song to be considered for recommendation."""

# Build the function to find top n songs

def top_n_songs(final_rating, n, min_interaction):
  recommendations = final_rating[final_rating['average_count'] > min_interaction]
  recommendations = recommendations.sort_values(by = 'average_count')
  
  return recommendations.index[:n]

# Recommend top 10 songs using the function defined above
list(top_n_songs(final_play, 10,1))

"""### **User User Similarity-Based Collaborative Filtering**

To build the user-user-similarity-based and subsequent models we will use the "surprise" library.
"""

# Install the surprise package using pip. Uncomment and run the below code to do the same
!pip install surprise

# Import necessary libraries

# To compute the accuracy of models
from surprise import accuracy

# This class is used to parse a file containing play_counts, data should be in structure - user; item; play_count
from surprise.reader import Reader

# Class for loading datasets
from surprise.dataset import Dataset

from collections import deque
import random

# For tuning model hyperparameters
from surprise.model_selection import GridSearchCV

# For splitting the data in train and test dataset
from surprise.model_selection import train_test_split

# For implementing similarity-based recommendation system
from surprise.prediction_algorithms.knns import KNNBasic

# For implementing matrix factorization based recommendation system
from surprise.prediction_algorithms.matrix_factorization import SVD

# For implementing KFold cross-validation
from surprise.model_selection import KFold

# For implementing clustering-based recommendation system
from surprise import CoClustering

"""### Some useful functions

Below is the function to calculate precision@k and recall@k, RMSE and F1_Score@k to evaluate the model performance.

**Think About It:** Which metric should be used for this problem to compare different models?
"""

# The function to calulate the RMSE, precision@k, recall@k, and F_1 score
def precision_recall_at_k(model, k = 30, threshold = 1.5):
    """Return precision and recall at k metrics for each user"""

    # First map the predictions to each user.
    user_est_true = defaultdict(list)
    
    # Making predictions on the test data
    predictions=model.test(testset)
    
    for uid, _, true_r, est, _ in predictions:
        user_est_true[uid].append((est, true_r))

    precisions = dict()
    recalls = dict()
    for uid, user_ratings in user_est_true.items():

        # Sort user ratings by estimated value
        user_ratings.sort(key = lambda x : x[0], reverse = True)

        # Number of relevant items
        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)

        # Number of recommended items in top k
        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[ : k])

        # Number of relevant and recommended items in top k
        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))
                              for (est, true_r) in user_ratings[ : k])

        # Precision@K: Proportion of recommended items that are relevant
        # When n_rec_k is 0, Precision is undefined. We here set Precision to 0 when n_rec_k is 0

        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0

        # Recall@K: Proportion of relevant items that are recommended
        # When n_rel is 0, Recall is undefined. We here set Recall to 0 when n_rel is 0

        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0
    
    # Mean of all the predicted precisions are calculated
    precision = round((sum(prec for prec in precisions.values()) / len(precisions)), 3)

    # Mean of all the predicted recalls are calculated
    recall = round((sum(rec for rec in recalls.values()) / len(recalls)), 3)
    
    accuracy.rmse(predictions)

    # Command to print the overall precision
    print('Precision: ', precision)

    # Command to print the overall recall
    print('Recall: ', recall)
    
    # Formula to compute the F-1 score
    print('F_1 score: ', round((2 * precision * recall) / (precision + recall), 3))

"""**Think About It:** In the function precision_recall_at_k above the threshold value used is 1.5. How precision and recall are affected by changing the threshold? What is the intuition behind using the threshold value of 1.5? """

# Instantiating Reader scale with expected rating scale 
reader = Reader(rating_scale = (0,5)) #use rating scale (0,5)

# loading the dataset
data = Dataset.load_from_df(result_final[["user_id", "song_id", "play_count"]], reader) #Take only "user_id","song_id", and "play_count"

# splitting the data into train and test dataset
trainset, testset = train_test_split(data, test_size=.4, random_state=42) # Take test_size=0.4

"""**Think About It:** How changing the test size would change the results and outputs?"""

# Build the default user-user-similarity model
sim_options = {'name': 'cosine',
               'user_based': False}

# KNN algorithm is used to find desired similar items
sim_user_user_optimized = KNNBasic(sim_options=sim_options, random_state=1) # Use random_state = 1 

# Train the algorithm on the trainset, and predict play_count for the testset
sim_user_user_optimized.fit(trainset)

# Let us compute precision@k, recall@k, and f_1 score with k = 10
precision_recall_at_k(sim_user_user_optimized) # Use sim_user_user model

"""**Observations and Insights:
The baseline model is giving a good F_1 score of ~ 45%. We will try to **improve this later by using GridSearchCV** by tuning different hyperparameters of this algorithm.
"""

# Predicting play_count for a sample user with a listened song
sim_user_user_optimized.predict("159", "1671", r_ui = 2, verbose = True) # Use user id 6958 and song_id 1671

# Predicting play_count for a sample user with a song not-listened by the user
sim_user_user_optimized.predict("159", "", verbose = True) # Use user_id 6958 and song_id 3232

"""**Observations and Insights:

The above output shows that **the actual rating for this user pair is 2, and the predicted rating is 2.33** by this **user-user-similarity-based baseline model**.

However, a user without having heard the song gets a rating estimate of 2.33

Now, let's try to tune the model and see if we can improve the model performance.
"""

# Setting up parameter grid to tune the hyperparameters
param_grid = {'k': [10, 20, 30], 'min_k': [3, 6, 9],
              'sim_options': {'name': ["cosine", 'pearson', "pearson_baseline"],
                              'user_based': [True], "min_support": [2, 4]}
              }

# Performing 3-fold cross-validation to tune the hyperparameters
gs = GridSearchCV(KNNBasic, param_grid, measures = ['rmse'], cv = 3, n_jobs = -1)

# Fitting the data
gs.fit(data) 

# Best RMSE score
print(gs.best_score["rmse"])

# Combination of parameters that gave the best RMSE score
print(gs.best_params["rmse"])

# Train the best model found in above gridsearch
print(gs.best_params)

"""**Observations and Insights:

When there are considerations other than maximum score when choosing a best model, you can set the refit to a function that returns the selected best_score. In that case, best_score_ and best_params_ will be set according to the best_index_ returned.
"""

# Predict the play count for a user who has listened to the song. Take user_id 6958, song_id 1671 and r_ui = 2
sim_user_user_optimized.predict("159", "1671", r_ui = 2, verbose = True)
sim_user_user_optimized

# Predict the play count for a song that is not listened to by the user (with user_id 6958)
sim_user_user_optimized.predict("159", "215", verbose = True)
sim_user_user_optimized

"""**Observations and Insights:

**Think About It:** Along with making predictions on listened and unknown songs can we get 5 nearest neighbors (most similar) to a certain song?
"""

# Use inner id 0
sim_user_user_optimized.get_neighbors(0, 5)

"""Below we will be implementing a function where the input parameters are:

- data: A **song** dataset
- user_id: A user-id **against which we want the recommendations**
- top_n: The **number of songs we want to recommend**
- algo: The algorithm we want to use **for predicting the play_count**
- The output of the function is a **set of top_n items** recommended for the given user_id based on the given algorithm
"""

result_final.head()

result_final

def get_recommendations(data, user_id, top_n, algo):
    
    # Creating an empty list to store the recommended product ids
    recommendations = []
    
    # Creating an user item interactions matrix 
    user_item_interactions_matrix = data.pivot_table(index = 'user_id', columns = 'song_id', values = 'play_count')
    
    # Extracting those business ids which the user_id has not visited yet
    non_interacted_products = user_item_interactions_matrix.loc[user_id][user_item_interactions_matrix.loc[user_id].isnull()].index.tolist()
    
    # Looping through each of the business ids which user_id has not interacted yet
    for item_id in non_interacted_products:
        
        # Predicting the ratings for those non visited restaurant ids by this user
        est = algo.predict(user_id, item_id).est
        
        # Appending the predicted ratings
        recommendations.append((item_id, est))

    # Sorting the predicted ratings in descending order
    recommendations.sort(key = lambda x : x[1], reverse = True)

    return recommendations[:top_n] # Returing top n highest predicted rating products for this user

# Make top 5 recommendations for user_id 6958 with a similarity-based recommendation engine
recommendations = get_recommendations(result_final, 159, 5, sim_user_user_optimized)
recommendations

# Building the dataframe for above recommendations with columns "song_id" and "predicted_ratings"
pd.DataFrame(recommendations, columns = ['song_id','play_count'])

"""**Observations and Insights:

In this table you can see the recommendations according to the song_id and the predictions_score.

### Correcting the play_counts and Ranking the above songs
"""

final_play

def ranking_songs(recommendations, final_play):
  # Sort the songs based on play counts
  ranked_songs = final_play.loc[[items[0] for items in recommendations]].sort_values('play_count', ascending = False)[['play_count']].reset_index()

  # Merge with the recommended songs to get predicted play_count
  ranked_songs = ranked_songs.merge(pd.DataFrame(recommendations, columns = ['song_id', 'predicted_count']), on = 'song_id', how = 'inner')

  # Rank the songs based on corrected play_counts
  ranked_songs['corrected_count'] = ranked_songs['predicted_count'] - 1 / np.sqrt(ranked_songs['play_count'])

  # Sort the songs based on corrected play_counts
  ranked_songs = ranked_songs.sort_values('corrected_count', ascending = False)
  
  return ranked_songs

"""**Think About It:** In the above function to correct the predicted play_count a quantity 1/np.sqrt(n) is subtracted. What is the intuition behind it? Is it also possible to add this quantity instead of subtracting?"""

recommendations

final_play

# Applying the ranking_songs function on the final_play data
ranking_songs(recommendations, final_play)

"""**Observations and Insights:
According to what was generated by ranking_songs, the predictions have been corrected.

### Item Item Similarity-based collaborative filtering recommendation systems
"""

# Apply the item-item similarity collaborative filtering model with random_state = 1 and evaluate the model performance
sim_options = {'name': 'cosine',
               'user_based': False}

# KNN algorithm is used to find desired similar items
sim_item_item = KNNBasic(sim_options = sim_options, random_state = 1, verbose = False)

# Train the algorithm on the train set, and predict ratings for the test set
sim_item_item.fit(trainset)

# Let us compute precision@k, recall@k, and f_1 score with k = 10
precision_recall_at_k(sim_item_item)

"""**Observations and Insights:
The baseline model is giving a good F_1 score of ~ 45%. We will try to **improve this later by using GridSearchCV** by tuning different hyperparameters of this algorithm.
"""

# Predicting play count for a sample user_id 6958 and song (with song_id 1671) heard by the user
sim_user_user_optimized.predict("159", "1671", r_ui = 2, verbose = True)
sim_user_user_optimized

# Predict the play count for a user that has not listened to the song (with song_id 1671)
sim_user_user_optimized.predict("645", "1671", verbose = True)
sim_user_user_optimized

"""**Observations and Insights:

According to what was analyzed in the first section "Predicting play count for a sample user_id 6958 and song (with song_id 1671) heard by the user" we verified in the csv. database which user had listened to song_id 1671 and was indeed the only user who had listened to the song. However, the only user who had not heard the song was 645.
"""

# Apply grid search for enhancing model performance

# Setting up parameter grid to tune the hyperparameters
param_grid = {'k': [10, 20, 30], 'min_k': [3, 6, 9],
              'sim_options': {'name': ["cosine", 'pearson', "pearson_baseline"],
                              'user_based': [False], "min_support": [2, 4]}
              }

# Performing 3-fold cross-validation to tune the hyperparameters
gs = GridSearchCV(KNNBasic, param_grid, measures = ['rmse'], cv = 3, n_jobs = -1)

# Fitting the data
gs.fit(data)

# Find the best RMSE score
print(gs.best_score['rmse'])

# Extract the combination of parameters that gave the best RMSE score
print(gs.best_params['rmse'])

"""**Think About It:** How do the parameters affect the performance of the model? Can we improve the performance of the model further? Check the list of hyperparameters [here](https://surprise.readthedocs.io/en/stable/knn_inspired.html)."""

# Apply the best modle found in the grid search
print(gs.best_params)

"""**Observations and Insights:

Applying this model, we will use the Best_params Algorithm. This algorithm computes the slope of each of the relevant items rated parameters, finds the difference.

"""

# Predict the play_count by a user(user_id 6958) for the song (song_id 1671)
sim_user_user_optimized.predict("159", "1671", r_ui = 2, verbose = True)
sim_user_user_optimized

# Predicting play count for a sample user_id 6958 with song_id 3232 which is not heard by the user
sim_user_user_optimized.predict("645", "1671", verbose = True)
sim_user_user_optimized

"""**Observations and Insights:

We have generated play count predictions for user "159" if he has listened to song_id with a prediction of 2.33, while user 645 has had no history of listening to song_id 1671.
"""

# Find five most similar items to the item with inner id 0
sim_item_item.get_neighbors(0, 5)

# Making top 5 recommendations for user_id 6958 with item_item_similarity-based recommendation engine
recommendations = get_recommendations(result_final, 159, 5, sim_user_user_optimized)

# Building the dataframe for above recommendations with columns "song_id" and "predicted_play_count"
pd.DataFrame(recommendations, columns = ['song_id','predicted_play_count'])

# Applying the ranking_songs function
ranking_songs(recommendations, final_play)

"""**Observations and Insights:

In this result, we had some predictions which the algorithm was able to correct according to the results of the recommendations.

 Now as we have seen **similarity-based collaborative filtering algorithms**, let us now get into **model-based collaborative filtering algorithms**.

### Model Based Collaborative Filtering - Matrix Factorization

Model-based Collaborative Filtering is a **personalized recommendation system**, the recommendations are based on the past behavior of the user and it is not dependent on any additional information. We use **latent features** to find recommendations for each user.
"""

# Build baseline model using svd
svd_algo_optimized = SVD()

# Making prediction for user (with user_id 6958) to song (with song_id 1671), take r_ui = 2
sim_user_user_optimized.predict("159", "1671", r_ui = 2, verbose = True)
sim_user_user_optimized

# Making a prediction for the user who has not listened to the song (song_id 3232)
sim_user_user_optimized.predict("645", "1671", verbose = True)
sim_user_user_optimized

"""#### Improving matrix factorization based recommendation system by tuning its hyperparameters"""

# Set the parameter space to tune
param_grid = {'n_epochs': [10, 20, 30], 'lr_all': [0.001, 0.005, 0.01],
              'reg_all': [0.2, 0.4, 0.6]}

# Performe 3-fold grid-search cross-validation
gs = GridSearchCV(KNNBasic, param_grid, measures = ['rmse'], cv = 3, n_jobs = -1)

# Fitting data
gs.fit(data)
# Best RMSE score
print(gs.best_score['rmse'])
# Combination of parameters that gave the best RMSE score
print(gs.best_params['rmse'])

"""**Think About It**: How do the parameters affect the performance of the model? Can we improve the performance of the model further? Check the available hyperparameters [here](https://surprise.readthedocs.io/en/stable/matrix_factorization.html)."""

# Building the optimized SVD model using optimal hyperparameters
svd_algo_optimized = SVD()

"""**Observations and Insights:

Training and evaluating SVD is very straightforward, similar to implementing the k-NN and baseline estimator when using Surprise. We just need to define the model with SVD()
"""

# Using svd_algo_optimized model to recommend for userId 6958 and song_id 1671
svd_algo_optimized = SVD()

# Training the algorithm on the trainset
svd_algo_optimized.fit(trainset)

# Use the function precision_recall_at_k to compute precision@k, recall@k, F1-Score, and RMSE
precision_recall_at_k(svd)

# Using svd_algo_optimized model to recommend for userId 6958 and song_id 3232 with unknown baseline rating
svd_algo_optimized.predict("159", "3232")

"""**Observations and Insights:
We have generated svd_algo_optimized.predict algorithm from the previous trainset.
"""

# Getting top 5 recommendations for user_id 6958 using "svd_optimized" algorithm
recommendations_user_id = get_recommendations(result_final, 159, 5, svd_algo_optimized)
recommendations_user_id

# Ranking songs based on above recommendations
ranking_songs(recommendations_user_id, final_play)

"""**Observations and Insights:
We have generated ranking_songs through the recommendation_user_id algorithm together with final_play dataset

### Cluster Based Recommendation System

In **clustering-based recommendation systems**, we explore the **similarities and differences** in people's tastes in songs based on how they rate different songs. We cluster similar users together and recommend songs to a user based on play_counts from other users in the same cluster.
"""

# Make baseline clustering model
CoCluster = CoClustering()

CoCluster.fit(trainset)

precision_recall_at_k(CoCluster)

# Making prediction for user_id 6958 and song_id 1671
sim_user_user_optimized.predict("159", "1671", r_ui = 2, verbose = True)
sim_user_user_optimized

# Making prediction for user (userid 6958) for a song(song_id 3232) not heard by the user
sim_user_user_optimized.predict("645", "3232", verbose = True)
sim_user_user_optimized

"""#### Improving clustering-based recommendation system by tuning its hyper-parameters"""

# Set the parameter space to tune
param_grid = {'n_cltr_u': [5, 6, 7, 8], 'n_cltr_i': [5, 6, 7, 8], 'n_epochs': [10, 20, 30]}

# Performing 3-fold grid search cross-validation
gs = GridSearchCV(KNNBasic, param_grid, measures = ['rmse'], cv = 3, n_jobs = -1)

# Fitting data
gs.fit(data)

# Best RMSE score
print(gs.best_score['rmse'])

# Combination of parameters that gave the best RMSE score
print(gs.best_params['rmse'])

"""**Think About It**: How do the parameters affect the performance of the model? Can we improve the performance of the model further? Check the available hyperparameters [here](https://surprise.readthedocs.io/en/stable/co_clustering.html)."""

# Train the tuned Coclustering algorithm
co_clustering_optimized = CoClustering()

co_clustering_optimized.fit(trainset)

precision_recall_at_k(CoCluster)

"""**Observations and Insights:
We observe that after tuning Coclustering algorithm, the model performance has not improved by much. We can try other values for parameters and see if we can get a better performance. However, here we will proceed with the existing model.
"""

# Using co_clustering_optimized model to recommend for userId 6958 and song_id 1671
co_clustering_optimized.predict("159", "1671")

# Use Co_clustering based optimized model to recommend for userId 6958 and song_id 3232 with unknown baseline rating
co_clustering_optimized.predict("645", "3232")

"""**Observations and Insights:

We have generated ranking_songs through the coclustering algorithm together with using user_id and song_id.

#### Implementing the recommendation algorithm based on optimized CoClustering model
"""

# Getting top 5 recommendations for user_id 6958 using "Co-clustering based optimized" algorithm
clustering_recommendations = get_recommendations(result_final, 159, 5, svd_algo_optimized)
clustering_recommendations

"""### Correcting the play_count and Ranking the above songs"""

# Ranking songs based on the above recommendations
ranking_songs(clustering_recommendations, final_play)

"""**Observations and Insights:

We have generated ranking_songs through the clustering algorithm together with final_play dataset.

### Content Based Recommendation Systems

**Think About It:** So far we have only used the play_count of songs to find recommendations but we have other information/features on songs as well. Can we take those song features into account?
"""

df_small = result_final

# Concatenate the "title", "release", "artist_name" columns to create a different column named "text"

result_final['Text'] = result_final['title'].map(str) + ' ' + result_final['release'].map(str)+ ' ' + result_final['artist_name'].map(str)
print(result_final["Text"])

# Select the columns 'user_id', 'song_id', 'play_count', 'title', 'text' from df_small data
df_small = result_final[["user_id", "song_id", "play_count", "title", "Text"]]

# Drop the duplicates from the title column
df_no_duplicates = df_small.drop_duplicates().T

# Set the title column as the index
df_no_duplicates = pd.DataFrame(df_small).set_index("title")

# See the first 5 records of the df_small dataset
df_small.head(5)

df_no_duplicates

# Create the series of indices from the data
indices = pd.Series(df_no_duplicates.all())
indices[ : 5]

# Importing necessary packages to work with text data
import nltk

# Download punkt library
nltk.download("punkt")

# Download stopwords library
nltk.download("stopwords")

# Download wordnet 
nltk.download("wordnet")

# Import regular expression
import re

# Import word_tokenizer
from nltk import word_tokenize

# Import WordNetLemmatizer
from nltk.stem import WordNetLemmatizer

# Import stopwords
from nltk.corpus import stopwords

# Import CountVectorizer and TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

"""We will create a **function to pre-process the text data:**"""

# Function to tokenize the text
def tokenize(text):
    
    text = re.sub(r"[^a-zA-Z]"," ", text.lower())
    
    tokens = word_tokenize(text)
    
    words = [word for word in tokens if word not in stopwords.words("english")]  # Use stopwords of english
    
    text_lems = [WordNetLemmatizer().lemmatize(lem).strip() for lem in words]

    return text_lems

# Create tfidf vectorizer 
vectorizer = TfidfVectorizer()
# Fit_transfrom the above vectorizer on the text column and then convert the output into an array
X = vectorizer.fit_transform(df_no_duplicates)
print(X)

# Compute the cosine similarity for the tfidf above output
cosine_sim = cosine_similarity(X, X)
print(cosine_sim)

""" Finally, let's create a function to find most similar songs to recommend for a given song."""

# Function that takes in song title as input and returns the top 10 recommended songs
def recommendations(title, similar_songs):
    
    recommended_songs = []
    
    # Getting the index of the song that matches the title
    idx = indices[indices == title].index[0]

    # Creating a Series with the similarity scores in descending order
    score_series = pd.Series(similar_songs[idx]).sort_values(ascending = False)

    # Getting the indexes of the 10 most similar songs
    top_10_indexes = list(score_series.iloc[1 : 11].index)
    print(top_10_indexes)
    
    # Populating the list with the titles of the best 10 matching songs
    for i in top_10_indexes:
        recommended_songs.append(list(df_small.index)[i])
        
    return recommended_songs

"""Recommending 10 songs similar to Learn to Fly"""

# Make the recommendation for the song with title 'Learn To Fly'

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html '/content/drive/MyDrive/Colab Notebooks/Sheyla_Perez_Milestone_2_Recommendation_Systems_final.ipynb'

"""**Observations and Insights:

All recommendation techniques have their advantages and disadvantages, shows a comparison of different recommendation techniques. Collaborative filtering uses user correlations to recommend items that other similar users like. User profiling is only based on user ratings on items and no content knowledge is needed. This makes the technique extremely simple and comprehensive. Also, it has the ability to provide cross-songs_id recommendations (recommend songs that are significantly different from previously obtained items regarding contents). The scalability problem of collaborative filtering can be solved by using element-based collaborative filtering.

## **Conclusion and Recommendations:** 

For such reasons, For cases like this it's often use very simple and lightweight models for practical applications like Item Item/user-user Similarity-based collaborative filtering recommendation systems. And the field of Recommendation Systems is no exception: it might be better to use simple memory-based models than other alternatives models like SVD or TF-IDF, even though the latter shows superior test accuracy. Unfortunately, there is no clear answer to this - it has to try as many approaches as possible and find an optimal solution that satisfies the constraints of the problem.
"""